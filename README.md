# ‚ö° Ollama CLI

Una aplicaci√≥n de terminal sencilla, potente y sin dependencias (¬°excepto Ollama!) para chatear con modelos de lenguaje locales. Programada en Go.

![Logo de Ollama](https://ollama.com/public/ollama.png)

## üéØ Caracter√≠sticas Principales

* **Gesti√≥n Autom√°tica:** Inicia y detiene autom√°ticamente el servidor `ollama serve` al ejecutar y cerrar la aplicaci√≥n.
* **Selecci√≥n de Modelo:** Lista todos tus modelos locales y te permite elegir con cu√°l chatear.
* **Memoria (Contexto):** Mantiene el historial de la conversaci√≥n, permitiendo al modelo "recordar" lo que se ha dicho.
* **Personalizaci√≥n (System Prompt):** Utiliza un *prompt* de sistema para guiar la personalidad y el comportamiento del modelo (ej. "Eres un asistente llamado Gemma, NO ChatGPT").
* **Streaming en Tiempo Real:** Muestra las respuestas palabra por palabra, igual que en la web.
* **Interfaz Colorida:** Utiliza colores para una experiencia de usuario clara y agradable en la terminal.
* **Comandos en el Chat:**
    * `exit` o `quit`: Salir de la aplicaci√≥n.
    * `clear` o `reset`: Borrar el historial de la conversaci√≥n actual y empezar de cero.

## üì¶ Instalaci√≥n

Este proyecto est√° dise√±ado para ser ejecutado desde el c√≥digo fuente usando el stack de Go.

### Requisitos Previos

* **[Go (Golang)](https://go.dev/doc/install)** (versi√≥n 1.21 o superior)
* **[Ollama](https://ollama.com/)** instalado en tu sistema.
* Al menos un modelo descargado (ej: `ollama pull llama3`)

### Pasos

1.  Clona este repositorio:
    ```bash
    # Reemplaza [TU_USUARIO]/[TU_REPO] por tu URL de GitHub
    git clone [https://github.com/](https://github.com/)[TU_USUARIO]/[TU_REPO].git
    cd [TU_REPO]
    ```

2.  Instala las dependencias de Go (principalmente `fatih/color`):
    ```bash
    go mod tidy
    ```

## üöÄ Uso

Simplemente ejecuta la aplicaci√≥n desde la ra√≠z del proyecto. El programa se encargar√° de iniciar `ollama serve` por ti.

```bash
go run .
```

La aplicaci√≥n te guiar√°:

1.  Esperar√° a que el servidor de Ollama est√© listo.
2.  Te mostrar√° una lista de tus modelos locales para que elijas uno.
3.  ¬°Empezar√° el chat!

### Ejemplo de Sesi√≥n

```
$ go run .
‚úî ¬°Ollama est√° listo y respondiendo!

Modelos de Ollama disponibles localmente:
1. llama3:8b
2. gemma:2b
Elige un modelo (1-2): 2

      / \
     / _ \
     \/ \/

Modelo seleccionado: gemma:2b
System Prompt cargado. Escribe 'exit' o 'clear'.

>>> hola, me llamo Dani
IA: ¬°Hola Dani! Soy Gemma, ¬øen qu√© puedo ayudarte hoy?

>>> ¬øc√≥mo me llamo?
IA: Te llamas Dani.
```

## üõ†Ô∏è Archivos del Proyecto

* **`ollama-cli.go`**: El c√≥digo fuente principal de la aplicaci√≥n.
* **`go.mod` / `go.sum`**: Gesti√≥n de dependencias de Go.
* **`logos.json`**: Archivo de configuraci√≥n que almacena el arte ASCII para los logos de los modelos.

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Si tienes una idea para una mejora o has encontrado un bug:

1.  Haz un *Fork* del repositorio.
2.  Crea una nueva rama (`git checkout -b feature/nueva-mejora`).
3.  Haz tus cambios y haz *commit* (`git commit -m "feat: A√±adir nueva mejora"`).
4.  Haz *Push* a tu rama (`git push origin feature/nueva-mejora`).
5.  Abre un *Pull Request*.
